# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YNFDLubVCDhtozwWDz0MOEE9XDbyWEKM
"""

import torch
import numpy as np
import torchvision
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
from torch.utils.data.sampler import SubsetRandomSampler
from torch.utils.data.dataloader import DataLoader

# download the data and create a pytorch dataset using MNIST class from torchvision.datasets
dataset = MNIST(root='data/',download=True,transform=ToTensor())

# Commented out IPython magic to ensure Python compatibility.
img,label = dataset[0]
import matplotlib.pyplot as plt
# %matplotlib inline
plt.imshow(img[0])

# we define and use a function split_indices to pick a random 20% fraction of the images for validation set
def split_indices(n, val_pct):
  # determine size of the validation set
  n_val = int(val_pct*n)
  # create random permutation of 0 to n-1
  idxs = np.random.permutation(n)
  # pick first n_val indices for validation set
  return idxs[n_val:], idxs[:n_val]

train_indices, val_indices = split_indices(len(dataset), val_pct = 0.2)
print(len(train_indices),len(val_indices))
print('simple val indices: ', val_indices[:20])

batch_size = 100
# Training sampler and dataloader
train_sampler = SubsetRandomSampler(train_indices)
train_dl = DataLoader(dataset, batch_size, sampler=train_sampler)

# validation sampler and dataloader
val_sampler = SubsetRandomSampler(val_indices)
val_dl = DataLoader(dataset, batch_size, sampler=val_sampler)

import torch.nn.functional as F
import torch.nn as nn

class MnistModel(nn.Module):
  # feedforward neural network  with 1 hidden layer
  def __init__(self, in_size, hidden_size, out_size):
    super().__init__()
    # hidden layer
    self.linear1 = nn.Linear(in_size, hidden_size)
    # output layer
    self.linear2 = nn.Linear(hidden_size, out_size)
  
  def forward(self, xb):
    # Flatten the image tensors
    xb = xb.view(xb.size(0),-1)
    # get intermediate output using hidden layer
    out = self.linear1(xb)
    # Apply activation function
    out = F.relu(out)
    # get  prediction using output layer
    out = self.linear2(out)
    return out

"""We will create model that contains hidden layer with 32 activations"""

ninput_size = 784
num_classes = 10

model = MnistModel(input_size, hidden_size = 32, out_size = num_classes)

for t in model.parameters():
  print(t.shape)

for images, label in train_dl:
  print('image.shape', images.shape)
  outputs = model(images)
  loss = F.cross_entropy(outputs, label)
  print('loss:',loss.item())
  break
print('output.shape: ', outputs.shape)
print('sample.output :\n', outputs[:2].data)

torch.cuda.is_available()

def get_dafault_divice():
  # pick gpu if avaliable, else cpu
  if torch.cuda.is_available():
    return torch.divice('cuda')
  else:
    return torch.device('cpu')

device = get_dafault_divice()
device

def to_device(data, device):
  # move tensor to chossen device
  if isinstance(data, (list,tuple)):
    return [to_device(x,device) for x in data]
  return data.to(device, non_blocking = True)

for images, labels in train_dl:
  print(images.shape)
  images = to_device(images, device)
  print(images.device)
  break

