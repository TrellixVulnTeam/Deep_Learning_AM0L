# -*- coding: utf-8 -*-
"""pytorch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qhK-8xs-DqfPYqdzvEpa5krWBgiAPUUO

importing pytorch
"""

import torch

"""**Pytorch**:- it is a library for processing tensors
**Tensors**:-it is a number vector, matrix, or any n dimension array
"""

# create a tensors with single number
t1 = torch.tensor(1000)
t1

# we can use dtype for knowing attribut of our tensors
t1.dtype

# creating vector using tensors
t2 = torch.tensor([1,2,3,4,5])
t2

# creating matrix using tensors
t3 = torch.tensor([[1., 2], [3, 4], [5, 6]])
t3

# using shape we can find the size of tensors
t1.shape
t2.shape
t3.shape

"""**Tensor operation and gradients**"""

# create tensor
x = torch.tensor(3.)
w = torch.tensor(4., requires_grad = True)
b = torch.tensor(5., requires_grad = True)

# Arithmatic operations
y = w * x + b
y

# compute derivative of y
y.backward()
y

"""derivatives of y w.r.t input tensors are strored in .grad property
gradient is use when we deal with matrix and derivative is used  when we
deal with number
"""

print("dy/dx: ", x.grad)

print("dy/dw: ", w.grad)

print("dy/db: ",b.grad)

"""**Interoperability with numpy**

Numpy:- numpy is a popular open source library used for mathematical and scientific computing in python

creating array in numpy
"""

import numpy as np

x = np.array([[1, 2],[3, 4]])
x

# convert numpy array to a torch tensor
y = torch.from_numpy(x)
y

x.dtype

y.dtype

# convert torch tensor in numpy array
z = y.numpy()
z

"""**Linear Regression**

**Gradient descent**:- it is a optimization technique which is used to adjusting the weights slightly to make better prediction
"""

import numpy as np
import torch

"""**Traning data**"""

#input (temp,rainfall,humidity)
input = np.array([[73, 67, 43],
                  [91, 88, 64],
                  [87, 134, 58],
                  [102, 43, 37],
                  [69, 96, 70]], dtype = 'float32')

# target(apples, orange)
target = np.array([[56, 70],
                   [81, 101],
                   [119, 133],
                   [22, 37],
                   [103, 119]], dtype = 'float32')

# convert input and target to tensors
inputs = torch.from_numpy(input)
targets = torch.from_numpy(target)
print(inputs)
print(targets)

# weights and biases
w = torch.randn(2, 3, requires_grad = True)
b = torch.randn(2, requires_grad = True)
print(w)
print(b)

def model(x):
  return x @ w.t() + b

# generate the predictions
pred = model(inputs)
print(pred)

print(targets)

"""**mean squard error**
1. calculate the difference between two matrix (target,pred)
2. square all the element of difference matrix (remove neg value)
3. calculate the average of elements in resulting matrix
"""

# MSE loss
def mse(t1, t2):
  diff = t1 - t2
  return torch.sum(diff * diff) / diff.numel()

# compute loss
loss = mse(targets,pred)
print(loss)

# compute gradients
loss.backward()

print(w)
print(w.grad)

w.grad.zero_()
b.grad.zero_()
print(w.grad)
print(b.grad)

"""**Adjust weights and biases using gradient descent**
we will reduce the loss and improve our model using gradient descent algorithm
which has following steps
1. generate prediction
2. calculate the loss
3. compute gradients w.r.t weights and biases
4. adjust the weights by subtracting small quantity proportional to the gradient
5. reset the gradients to zero
"""

# generate the predictions
pred = model(inputs)
print(pred)

# calculate the loss
loss = mse(pred, targets)
print(loss)

# compute gradient
loss.backward()
print(w.grad)
print(b.grad)

# Adjust weights and reset gradient to zero
with torch.no_grad():
  w -= w.grad * 1e-5
  b -= b.grad * 1e-5
  w.grad.zero_()
  b.grad.zero_()

print(w)
print(b)

# calculate the loss
pred = model(inputs)
loss = mse(pred, targets)
print(loss)

# Train for 100 epochs
for i in range(100):
  pred = model(inputs)
  loss = mse(pred, targets)
  loss.backward()
  with torch.no_grad():
    w -= w.grad * 1e-5
    b -= b.grad * 1e-5
    w.grad.zero_()
    b.grad.zero_()

# calculate the loss
pred = model(inputs)
loss = mse(pred, targets)
print(loss)

print(pred)

print(targets)

"""**Linear regression using pytorch builtin**"""

import torch.nn as nn

#input (temp,rainfall,humidity)
input1 = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58],
                  [91, 88, 64],  [69, 96, 70], [73, 67, 43],
                  [87, 134, 58], [87, 134, 58], [102, 43, 37],
                  [102, 43, 37], [73, 67, 43], [91, 88, 64],
                  [69, 96, 70],  [102, 43, 37], [69, 96, 70]],dtype = 'float32')
input1 = torch.from_numpy(input1)

# target(apples, orange)
target1 = np.array([[56, 70],[81, 101],[119, 133],
                   [81, 101],[103, 119], [56, 70],
                   [119, 133],[119, 133], [22, 37],
                   [22, 37],[56, 70], [81, 101],
                   [103, 119],[22, 37], [103, 119]], dtype = 'float32')
target1 = torch.from_numpy(target1)

print(input1)
print(target1)

"""**Dataset and Dataloder**"""

from torch.utils.data import TensorDataset

# Define dataset
train_ds = TensorDataset(input1, target1)
train_ds[0:3]

from torch.utils.data import DataLoader

# Define dataloder
batch_size = 5
train_dl = DataLoader(train_ds, batch_size, shuffle= "True")

for xy, xb in train_dl:
  print(xy)
  print(xb)
  break

"""**nn.Linear**
Instead of initializing the weights and biases manually, we can define the model
using nn.Linear class from pytorch, which does it automatically
"""

# define model
model = nn.Linear(3,2)
print(model.weight)
print(model.bias)

# generate prediction
pred1 = model(input1)

pred1

# import nn.functional 
import torch.nn.functional as F

# Define loss function
loss_fn = F.mse_loss

# compute the loss for our current prediction
loss = loss_fn(model(input1), target1)
print(loss)

# Define SGD optimizer
opt = torch.optim.SGD(model.parameters(), lr = 1e-5)

"""**Train the model**"""

# Utility function to train the model
def fit(num_epoch, model, loss_fn, opt):

  # repeat the given number of epoch
  
  for i in range(num_epoch):
   
    # Train with batches of data
   
    for xb, yb in train_dl:

      # generate prediction
      pred = model(xb)

      # calculate loss
      loss = loss_fn(pred, yb)

      # compute gradient
      loss.backward()

      # update parameter using gradient
      opt.step()

      # reset the gradient to zero
      opt.zero_grad()
    if (i+1)%10 == 0:
        print('Epoch [{}/{}], loss: {:.4f}'.format(i+1, num_epoch, loss.item()))

fit(100, model, loss_fn, opt)

# Generate prediction
pred = model(input1)
print(pred)

print(target1)

